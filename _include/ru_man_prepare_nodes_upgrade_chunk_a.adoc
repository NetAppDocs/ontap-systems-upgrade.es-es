= 
:allow-uri-read: 


Antes de poder sustituir los nodos originales, debe confirmar que están en un par de alta disponibilidad, que no faltan discos o que han fallado, que pueden acceder al almacenamiento de otros y que no tienen LIF de datos asignadas a los otros nodos del clúster. También debe recopilar información acerca de los nodos originales y, si el clúster se encuentra en un entorno SAN, confirmar que todos los nodos del clúster están en quórum.

.Pasos
. Confirme que cada uno de los nodos originales tiene suficientes recursos para admitir adecuadamente la carga de trabajo de ambos nodos durante el modo de toma de control.
+
Consulte link:other_references.html["Referencias"]el enlace _HA PARK MANAGEMENT_ y siga la sección _Best practices for HA PARES_. Ninguno de los nodos originales debe estar funcionando con una utilización superior al 50 %; si el nodo se está ejecutando con una utilización inferior al 50 %, puede manejar las cargas de ambos nodos durante la actualización de la controladora.

. Complete los siguientes subpasos para crear una base de rendimiento para los nodos originales:
+
.. Asegúrese de que la cuenta de usuario de diagnóstico está desbloqueada.
+
[IMPORTANT]
====
La cuenta de usuario de diagnóstico está diseñada únicamente para fines de diagnóstico de bajo nivel y sólo se debe utilizar con la orientación del soporte técnico.

Para obtener información acerca de cómo desbloquear las cuentas de usuario, consulte link:other_references.html["Referencias"] Para vincular a _System Administration Reference_.

====
.. Consulte link:other_references.html["Referencias"] Para establecer un enlace al _sitio de soporte de NetApp_ y descargar el recopilador de rendimiento y estadísticas (Perfstat Converged).
+
La herramienta convergente Perfstat le permite establecer una base de rendimiento para realizar la comparación después de la actualización.

.. Cree una línea de rendimiento de base, siguiendo las instrucciones que se encuentran en el sitio de soporte de NetApp.


. Consulte link:other_references.html["Referencias"] Para enlazar con el _sitio de soporte de NetApp_ y abrir un caso de soporte en el sitio de soporte de NetApp.
+
Puede utilizar este caso para informar de los problemas que puedan surgir durante la actualización.

. Verifique que se carguen las baterías de NVMEM o NVRAM de nodo 3 y nodo 4 y carguen si no lo son.
+
Debe comprobar físicamente los nodos 3 y 4 para ver si se cargan las baterías de NVMEM o NVRAM. Para obtener información sobre los LED del modelo de nodo 3 y nodo 4, consulte link:other_references.html["Referencias"] Para enlazar con _Hardware Universe_.

+

WARNING: No intente borrar el contenido de la NVRAM .  Si es necesario borrar el contenido de la NVRAM, comuníquese con el soporte técnico de NetApp .

. Comprobar la versión de ONTAP en los nodos 3 y 4.
+
Los nodos nuevos deben tener la misma versión de ONTAP 9.x instalada en ellos que se haya instalado en los nodos originales. Si los nodos nuevos tienen otra versión de ONTAP instalada, debe reiniciar las controladoras nuevas después de instalarlas. Para obtener instrucciones sobre cómo actualizar ONTAP, consulte link:other_references.html["Referencias"] Para enlazar a _Upgrade ONTAP_.

+
En las cajas de envío, se debe incluir información sobre la versión de ONTAP en los nodos 3 y 4. La versión de ONTAP se muestra cuando el nodo arranca o puede arrancar el nodo en modo de mantenimiento y ejecutar el comando:

+
`version`

. Compruebe si tiene dos o cuatro LIF de clúster en los nodos 1 y 2:
+
`network interface show -role cluster`

+
El sistema muestra cualquier LIF del clúster, como se muestra en el ejemplo siguiente:

+
....
cluster::> network interface show -role cluster
        Logical    Status     Network          Current  Current Is
Vserver Interface  Admin/Oper Address/Mask     Node     Port    Home
------- ---------- ---------- ---------------- -------- ------- ----
node1
        clus1      up/up      172.17.177.2/24  node1    e0c     true
        clus2      up/up      172.17.177.6/24  node1    e0e     true
node2
        clus1      up/up      172.17.177.3/24  node2    e0c     true
        clus2      up/up      172.17.177.7/24  node2    e0e     true
....
. Si tiene dos o cuatro LIF de clúster en los nodos 1 o 2, asegúrese de poder hacer el ping de ambos LIF de clúster en todas las rutas disponibles completando los siguientes subpasos:
+
.. Introduzca el nivel de privilegio avanzado:
+
`set -privilege advanced`

+
El sistema muestra el siguiente mensaje:

+
....
Warning: These advanced commands are potentially dangerous; use them only when directed to do so by NetApp personnel.
Do you wish to continue? (y or n):
....
.. Introduzca `y`.
.. Haga ping en los nodos y pruebe la conectividad:
+
`cluster ping-cluster -node node_name`

+
El sistema muestra un mensaje similar al siguiente ejemplo:

+
....
cluster::*> cluster ping-cluster -node node1
Host is node1
Getting addresses from network interface table...
Local = 10.254.231.102 10.254.91.42
Remote = 10.254.42.25 10.254.16.228
Ping status:
...
Basic connectivity succeeds on 4 path(s) Basic connectivity fails on 0 path(s)
................
Detected 1500 byte MTU on 4 path(s):
Local 10.254.231.102 to Remote 10.254.16.228
Local 10.254.231.102 to Remote 10.254.42.25
Local 10.254.91.42 to Remote 10.254.16.228
Local 10.254.91.42 to Remote 10.254.42.25
Larger than PMTU communication succeeds on 4 path(s)
RPC status:
2 paths up, 0 paths down (tcp check)
2 paths up, 0 paths down (udp check)
....
+
Si el nodo utiliza dos puertos de clúster, debe ver que puede comunicarse en cuatro rutas, como se muestra en el ejemplo.

.. Devolver al privilegio de nivel administrativo:
+
`set -privilege admin`



. Confirme que los nodos 1 y 2 están en un par de alta disponibilidad y compruebe que los nodos están conectados entre sí y que la toma de control es posible:
+
`storage failover show`

+
En el siguiente ejemplo, se muestra el resultado cuando los nodos están conectados entre sí y es posible la toma de control:

+
....
cluster::> storage failover show
                              Takeover
Node           Partner        Possible State Description
-------------- -------------- -------- -------------------------------
node1          node2          true     Connected to node2
node2          node1          true     Connected to node1
....
+
Ninguno de los nodos debe estar en una devolución parcial. El siguiente ejemplo muestra que el nodo 1 está en una devolución parcial:

+
....
cluster::> storage failover show
                              Takeover
Node           Partner        Possible State Description
-------------- -------------- -------- -------------------------------
node1          node2          true     Connected to node2, Partial giveback
node2          node1          true     Connected to node1
....
+
Si alguno de los nodos está en retorno parcial, utilice `storage failover giveback` el comando para realizar el retorno al nodo primario y, a continuación, utilice `storage failover show-giveback` el comando para asegurarse de que aún no es necesario devolver agregados. Para obtener información detallada sobre los comandos, consulte link:other_references.html["Referencias"]el enlace a _HA pair management_.

. [[man_prepare_Nodes_step9]]confirme que ni el nodo 1 ni el nodo 2 poseen los agregados para los que es el propietario actual (pero no el propietario del hogar):
+
`storage aggregate show -nodes _node_name_ -is-home false -fields owner-name, home-name, state`

+
Si ni el nodo 1 ni el nodo 2 tienen agregados cuyos propietarios son actuales (pero no el propietario del hogar), el sistema devolverá un mensaje similar al siguiente ejemplo:

+
....
cluster::> storage aggregate show -node node2 -is-home false -fields owner-name,homename,state
There are no entries matching your query.
....
+
En el siguiente ejemplo, se muestra el resultado del comando para un nodo con el nombre 2, que es el propietario raíz, pero no el propietario actual, de cuatro agregados:

+
....
cluster::> storage aggregate show -node node2 -is-home false
               -fields owner-name,home-name,state

aggregate     home-name    owner-name   state
------------- ------------ ------------ ------
aggr1         node1        node2        online
aggr2         node1        node2        online
aggr3         node1        node2        online
aggr4         node1        node2        online

4 entries were displayed.
....
. Realice una de las siguientes acciones:
+
[cols="35,65"]
|===
| Si el comando de <<man_prepare_nodes_step9,Paso 9>>... | Realice lo siguiente... 


| Tenía salida en blanco | Vaya al paso 11 y vaya a. <<man_prepare_nodes_step12,Paso 12>>. 


| Tenía salida | Vaya a. <<man_prepare_nodes_step11,Paso 11>>. 
|===
. [[man_prepare_Nodes_step11]] Si el nodo 1 o el nodo 2 tienen agregados cuyos propietarios son actuales, pero no el propietario raíz, complete los siguientes subpasos:
+
.. Devolver los agregados que actualmente pertenecen al nodo asociado al nodo propietario principal:
+
`storage failover giveback -ofnode _home_node_name_`

.. Compruebe que ni el nodo 1 ni el nodo 2 siguen teniendo agregados cuyos propietarios son actualmente (pero no el propietario del hogar):
+
`storage aggregate show -nodes _node_name_ -is-home false -fields owner-name, home-name, state`

+
En el ejemplo siguiente se muestra el resultado del comando cuando un nodo es al mismo tiempo el propietario actual y el propietario principal de los agregados:

+
....
cluster::> storage aggregate show -nodes node1
          -is-home true -fields owner-name,home-name,state

aggregate     home-name    owner-name   state
------------- ------------ ------------ ------
aggr1         node1        node1        online
aggr2         node1        node1        online
aggr3         node1        node1        online
aggr4         node1        node1        online

4 entries were displayed.
....


. [[man_prepare_Nodes_step12]] confirmar que los nodos 1 y 2 pueden acceder entre sí al almacenamiento y comprobar que no faltan discos:
+
`storage failover show -fields local-missing-disks,partner-missing-disks`

+
El ejemplo siguiente muestra el resultado cuando no hay discos:

+
....
cluster::> storage failover show -fields local-missing-disks,partner-missing-disks

node     local-missing-disks partner-missing-disks
-------- ------------------- ---------------------
node1    None                None
node2    None                None
....
+
Si falta algún disco, consulte link:other_references.html["Referencias"]el enlace a _Disk and Aggregate management con CLI_, _Logical storage management con CLI_ y _HA pair management_ para configurar el almacenamiento para la pareja de alta disponibilidad.

. Confirmar que los nodos 1 y 2 están en buen estado y que pueden participar en el clúster:
+
`cluster show`

+
En el siguiente ejemplo se muestra el resultado cuando ambos nodos son elegibles y están en buen estado:

+
....
cluster::> cluster show

Node                  Health  Eligibility
--------------------- ------- ------------
node1                 true    true
node2                 true    true
....
. Configure el nivel de privilegio en Advanced:
+
`set -privilege advanced`

. [[man_prepare_Nodes_step15]]] confirme que los nodos 1 y 2 ejecutan la misma versión de ONTAP:
+
`system node image show -node _node1,node2_ -iscurrent true`

+
En el siguiente ejemplo se muestra el resultado del comando:

+
....
cluster::*> system node image show -node node1,node2 -iscurrent true

                 Is      Is                Install
Node     Image   Default Current Version   Date
-------- ------- ------- ------- --------- -------------------
node1
         image1  true    true    9.1         2/7/2017 20:22:06
node2
         image1  true    true    9.1         2/7/2017 20:20:48

2 entries were displayed.
....
. Compruebe que ni el nodo 1 ni el nodo 2 tienen a sus LIF de datos que pertenecen a otros nodos del clúster y compruebe el `Current Node` y.. `Is Home` columnas de la salida:
+
`network interface show -role data -is-home false -curr-node _node_name_`

+
El ejemplo siguiente muestra el resultado cuando el nodo 1 no tiene ninguna LIF propietaria de otros nodos del clúster:

+
....
cluster::> network interface show -role data -is-home false -curr-node node1
 There are no entries matching your query.
....
+
En el ejemplo siguiente se muestra el resultado cuando el nodo 1 tiene las LIF de datos propias del otro nodo:

+
....
cluster::> network interface show -role data -is-home false -curr-node node1

            Logical    Status     Network            Current       Current Is
Vserver     Interface  Admin/Oper Address/Mask       Node          Port    Home
----------- ---------- ---------- ------------------ ------------- ------- ----
vs0
            data1      up/up      172.18.103.137/24  node1         e0d     false
            data2      up/up      172.18.103.143/24  node1         e0f     false

2 entries were displayed.
....
. Si la salida en <<man_prepare_nodes_step15,Paso 15>> Muestra que los nodos 1 o 2 tienen a su propio propietario cualquier LIF de datos que otros nodos del clúster, migre las LIF de datos del nodo 1 o del nodo 2:
+
`network interface revert -vserver * -lif *`

+
Para obtener información detallada acerca de `network interface revert` consulte link:other_references.html["Referencias"] Para enlazar a los comandos _ONTAP 9: Manual Page Reference_.

. Compruebe si el nodo 1 o el nodo 2 tienen algún disco con errores:
+
`storage disk show -nodelist _node1,node2_ -broken`

+
Si alguno de los discos ha fallado, extráigalos siguiendo las instrucciones de la gestión de _Disk y aggregate con la CLI_. (Consulte link:other_references.html["Referencias"] Para enlazar con _Disk y aggregate Management con la CLI_).

. Recopile información acerca de los nodos 1 y 2 completando los siguientes subpasos y grabando la salida de cada comando:

