---
sidebar: sidebar 
permalink: upgrade-arl-auto/relocate_non_root_aggr_nas_lifs_from_node2_to_node3.html 
keywords: relocate, non-root, aggregates, nas, lif, node2, node3 
summary: 'Reubique los agregados que no son raíz del nodo 3 cuando actualice las controladoras que ejecutan ONTAP 9.5 a 9.7 mediante `system controller replace` comandos.' 
---
= Reubique los agregados que no son raíz y los LIF de datos NAS del nodo 2 al nodo 3
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
Antes de sustituir node2 con node4, debe reubicar los agregados no raíz y los LIF de datos NAS que node2 posee a node3.

.Antes de empezar
Una vez completadas las comprobaciones posteriores a la fase anterior, la versión de recursos para el nodo 2 se iniciará automáticamente. Los agregados que no son raíz y los LIF de datos que no son SAN se migran del nodo 2 al nodo 3.

.Acerca de esta tarea
Las LIF remotas gestionan el tráfico a LUN DE SAN durante el procedimiento de actualización. No es necesario mover LIF DE SAN durante la actualización para el estado del clúster o del servicio.

Una vez migrados los agregados y las LIF, la operación se coloca en pausa con fines de verificación. En este momento, debe verificar si todos los agregados que no son raíz y los LIF de datos no SAN se migran al nodo 3.


NOTE: El propietario raíz de los agregados y las LIF no se modifica; solo se modifica el propietario actual.

.Pasos
. Verifique que todos los agregados que no son raíz estén en línea y su estado en el nodo 3:
+
`storage aggregate show -node <node3> -state online -root false`

+
En el ejemplo siguiente, los agregados no raíz en el nodo 2 están en línea:

+
....
cluster::> storage aggregate show -node node3 state online -root false

Aggregate      Size         Available   Used%   State   #Vols  Nodes  RAID     Status
----------     ---------    ---------   ------  -----   -----  ------ -------  ------
aggr_1         744.9GB      744.8GB      0%     online  5      node2  raid_dp  normal
aggr_2         825.0GB      825.0GB      0%     online  1      node2  raid_dp  normal
2 entries were displayed.
....
+
Si los agregados se han desconectado o se han vuelto externos del nodo 3, conectarlos mediante el uso del siguiente comando de nodo 3, una vez por cada agregado:

+
`storage aggregate online -aggregate <aggregate_name>`

. Verifique que todos los volúmenes estén en línea en el nodo 3. Para ello, use el siguiente comando en el nodo 3 y examine el resultado:
+
`volume show -node <node3> -state offline`

+
Si alguno de los volúmenes se encuentra sin conexión en el nodo 3, debe volver a estar en línea mediante el siguiente comando de nodo 3, una vez para cada volumen:

+
`volume online -vserver <vserver_name> -volume <volume_name>`

+
El  `vserver_name` El uso de este comando se encuentra en la salida del comando anterior.  `volume show` dominio.

. Compruebe que las LIF se han movido a los puertos correctos y que tienen el estado de `up`. Si alguna LIF está inactiva, establezca el estado administrativo de las LIF en `up` Con el siguiente comando, una vez para cada LIF:
+
`network interface modify -vserver <vserver_name> -lif <LIF_name> -home-node <node_name> -status-admin up`

. Si los puertos que alojan actualmente las LIF de datos no existen en el nuevo hardware, elimínelas del dominio de retransmisión:
+
`network port broadcast-domain remove-ports`



. [[step5]]Compruebe que no quedan LIF de datos en el nodo 2 introduciendo el comando siguiente y examinando la salida:
+
`network interface show -curr-node _node2_ -role data`

. Si tiene configurados grupos de interfaces o VLAN, complete los siguientes subpasos:
+
.. Registre información de la VLAN y del grupo de interfaces para poder volver a crear las VLAN y los grupos de interfaces en el nodo 3 una vez que se arranca el nodo 3.
.. Quite las VLAN de los grupos de interfaces:
+
`network port vlan delete -node _nodename_ -port _ifgrp_ -vlan-id _VLAN_ID_`

.. Compruebe si hay algún grupo de interfaces configurado en el nodo introduciendo el siguiente comando y examinando su resultado:
+
`network port ifgrp show -node _node2_ -ifgrp _ifgrp_name_ -instance`

+
El sistema muestra información del grupo de interfaces del nodo, como se muestra en el ejemplo siguiente:

+
[listing]
----
cluster::> network port ifgrp show -node node2 -ifgrp a0a -instance
                 Node: node3
 Interface Group Name: a0a
Distribution Function: ip
        Create Policy: multimode_lacp
          MAC Address: 02:a0:98:17:dc:d4
   Port Participation: partial
        Network Ports: e2c, e2d
             Up Ports: e2c
           Down Ports: e2d
----
.. Si hay algún grupo de interfaces configurado en el nodo, registre los nombres de esos grupos y los puertos asignados a ellos y, a continuación, elimine los puertos introduciendo el comando siguiente, una vez para cada puerto:
+
`network port ifgrp remove-port -node _nodename_ -ifgrp _ifgrp_name_ -port _netport_`




